{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZxP90j--otU"
      },
      "outputs": [],
      "source": [
        "# Importing the NLTK library\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download_shell()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB4VXSIh-6cc",
        "outputId": "31e57190-9e72-4e21-90b2-267044d90301"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing WordNet \n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "o_GvNssFDWER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding some basic functions of synsets.\n",
        "\n",
        "Synset's name, lemmas, definitions and examples."
      ],
      "metadata": {
        "id": "7m093oVDbged"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the synset for the word \"game\"\n",
        "syns = wordnet.synsets('game')\n",
        "\n",
        "# Take the first word present in the above synstes\n",
        "syn = syns[0]\n",
        "\n",
        "# printing the synset name\n",
        "print (\"Synset name : \", syn.name(), '\\n')\n",
        "\n",
        "# print only the first word of the synset name\n",
        "print(\"Just the word :\", syn.lemmas()[0].name(), '\\n')\n",
        "\n",
        "# Definition of the word\n",
        "print (\"Synset meaning : \", syn.definition(), '\\n')\n",
        "\n",
        "# list of phrases that use the word in context\n",
        "print (\"Synset example : \", syn.examples())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Wa0y54FQ_x",
        "outputId": "de6ef4bf-f665-482d-c7e0-93aba0d899e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset name :  game.n.01 \n",
            "\n",
            "Just the word : game \n",
            "\n",
            "Synset meaning :  a contest with rules to determine a winner \n",
            "\n",
            "Synset example :  ['you need four people to play this game']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-Speech (PoS) in Synset."
      ],
      "metadata": {
        "id": "Yhwcx5rUkYzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the PoS tags of the following word \n",
        "syn = wordnet.synsets('game')[0]\n",
        "print (\"Syn tag : \", syn.pos())\n",
        " \n",
        "syn = wordnet.synsets('doing')[0]\n",
        "print (\"Syn tag : \", syn.pos())\n",
        " \n",
        "syn = wordnet.synsets('ugly')[0]\n",
        "print (\"Syn tag : \", syn.pos())\n",
        " \n",
        "syn = wordnet.synsets('slowly')[0]\n",
        "print (\"Syn tag : \", syn.pos())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm-f9RcXkeNR",
        "outputId": "84ffc8bc-ce45-4774-d4e6-c15752ae5071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Syn tag :  n\n",
            "Syn tag :  v\n",
            "Syn tag :  a\n",
            "Syn tag :  r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypernyms and Hyponyms"
      ],
      "metadata": {
        "id": "p2Ku58Qebvx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the synset for the word 'cat'\n",
        "syn_cat = wordnet.synsets('cat')[0]\n",
        "\n",
        "print(\"Synset name : \", syn_cat.name(), '\\n')\n",
        "\n",
        "# Getting hypernyms of the word 'cat'\n",
        "print(\"Synset hypernym : \", syn_cat.hypernyms(), '\\n')\n",
        "\n",
        "# Getting hyponyms of the word 'cat'\n",
        "print(\"Synset hyponyms : \", syn_cat.hypernyms()[0].hyponyms(), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euhuZG8QYQJe",
        "outputId": "83afaef9-4cee-42b9-cdc5-471aab65daf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset name :  cat.n.01 \n",
            "\n",
            "Synset hypernym :  [Synset('feline.n.01')] \n",
            "\n",
            "Synset hyponyms :  [Synset('big_cat.n.01'), Synset('cat.n.01')] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the root hypernym\n",
        "print(\"Synset root hypernyms : \", syn_cat.root_hypernyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS0gVWYwdji_",
        "outputId": "90749f4b-1de9-4999-e6be-115c65d75c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset root hypernyms :  [Synset('entity.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Synonyms and Antonyms"
      ],
      "metadata": {
        "id": "_xzwHrlieCDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating two empty list of synonyms and antonyms\n",
        "synonyms = [] \n",
        "antonyms = [] \n",
        "\n",
        "# looping throught the synonyms as syn for the synsets of the word \"love\"  \n",
        "for syn in wordnet.synsets(\"love\"): \n",
        "    for l in syn.lemmas():        # \"getting their lemmatized form\"\n",
        "        synonyms.append(l.name())     # appending the synonyms in the synonyms list\n",
        "        if l.antonyms():            # if there are antonyms of the syns\n",
        "            antonyms.append(l.antonyms()[0].name())   # appending antonyms in the antonyms list\n",
        "  \n",
        "print('Synonyns of \"love\" :', '\\n', set(synonyms)) \n",
        "print(\"\\n\")\n",
        "print(\"Antonyms of 'love' :\", set(antonyms)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYNFEE76d4iT",
        "outputId": "cfcf42f9-ee74-49c7-9031-494c88cc0a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyns of \"love\" : \n",
            " {'screw', 'get_laid', 'beloved', 'hump', 'making_love', 'honey', 'lie_with', 'have_it_away', 'jazz', 'make_out', 'sexual_love', 'love_life', 'sleep_with', 'dear', 'erotic_love', 'bang', 'enjoy', 'be_intimate', 'get_it_on', 'sleep_together', 'bonk', 'bed', 'do_it', 'make_love', 'have_intercourse', 'have_it_off', 'eff', 'have_a_go_at_it', 'love', 'lovemaking', 'roll_in_the_hay', 'dearest', 'fuck', 'know', 'have_sex', 'passion'}\n",
            "\n",
            "\n",
            "Antonyms of 'love' : {'hate'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Meronyms and Holonyms"
      ],
      "metadata": {
        "id": "-l-bkw10fSA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the meronyms of the word 'bicycle'\n",
        "bicycle = wordnet.synset('bicycle.n.01')\n",
        "print(bicycle.part_meronyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJKaR-QMfIAb",
        "outputId": "9ba2c9ff-fd57-4f7e-bfcb-12cdded3ca15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('bicycle_seat.n.01'), Synset('bicycle_wheel.n.01'), Synset('chain.n.03'), Synset('coaster_brake.n.01'), Synset('handlebar.n.01'), Synset('kickstand.n.01'), Synset('mudguard.n.01'), Synset('pedal.n.02'), Synset('sprocket.n.02')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the holonyms for the words \"atom\" and \"wood\"\n",
        "print(wordnet.synset('atom.n.01').part_holonyms(), '\\n')\n",
        "print(wordnet.synset('wood.n.01').substance_holonyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srzui4oVfiqv",
        "outputId": "03cc8cc9-016b-4178-9554-f7657ec4ff08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('chemical_element.n.01'), Synset('molecule.n.01')] \n",
            "\n",
            "[Synset('beam.n.02'), Synset('chopping_block.n.01'), Synset('lumber.n.01'), Synset('spindle.n.02')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entailments"
      ],
      "metadata": {
        "id": "YfBomIEuhxVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the entailments of the word \"walk\"\n",
        "print(wordnet.synset('walk.v.01').entailments())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PctCg8Pmgjs0",
        "outputId": "772a74e5-d621-423f-aa99-f2c93715a12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('step.v.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the entailments of the word \"eat\"\n",
        "print(wordnet.synset('eat.v.01').entailments())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOhYqq_Vhvk4",
        "outputId": "e790bb78-9ad4-4c44-f321-b6ae9adda1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('chew.v.01'), Synset('swallow.v.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity"
      ],
      "metadata": {
        "id": "Hv449U0CicPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare the noun of \"ship\" and \"boat:\"\n",
        "w1 = wordnet.synset('ship.n.01')\n",
        "w2 = wordnet.synset('boat.n.01')\n",
        "print(w1.wup_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VssXMMeeiRFg",
        "outputId": "e48c7d1f-510d-4f66-ef5e-499987a3a77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare the noun of \"ship\" and \"car:\"\n",
        "w1 = wordnet.synset('ship.n.01')\n",
        "w2 = wordnet.synset('car.n.01')\n",
        "print(w1.wup_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZVuuzRzjNFl",
        "outputId": "94b0a49f-a92c-49b1-c550-73fcd0a3f16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6956521739130435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare the noun of \"ship\" and \"dog:\"\n",
        "w1 = wordnet.synset('ship.n.01')\n",
        "w2 = wordnet.synset('dog.n.01')\n",
        "print(w1.wup_similarity(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6vH5u63jSGv",
        "outputId": "ea87aa6b-5d7c-4fc8-9932-70afaa60c8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n2wvLkAIjuti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
