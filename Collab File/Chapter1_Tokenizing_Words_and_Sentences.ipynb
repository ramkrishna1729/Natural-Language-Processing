{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEl6p8v5mq5N"
      },
      "outputs": [],
      "source": [
        "# Importing NLTK module\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading nltk files\n",
        "nltk.download_shell()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9-Km1X-m8mo",
        "outputId": "05c985ff-0057-4e71-d820-987ddf355349"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "Hit Enter to continue: \n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "Hit Enter to continue: x\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> x\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing tokenizers (word and senetence tokenizers)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "3xd8YjPYnkZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# firstly we use sentence tokenizer to tokenize our para\n",
        "para = \"Ram Krishna Pandey is the student of the college TMSL. He is right now LEARNING 'NATURAL LANGUAGE PROCESSING' with suvidha foundation. He lives in Kolakta      \""
      ],
      "metadata": {
        "id": "0w1tz5Fsob8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_para = sent_tokenize(para) \n",
        "print(token_para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJOA8ybfpVGQ",
        "outputId": "5d036625-1195-477c-9269-54aec1cf11fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ram Krishna Pandey is the student of the college TMSL.', \"He is right now LEARNING 'NATURAL LANGUAGE PROCESSING' with suvidha foundation.\", 'He lives in Kolakta']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we want to tokenize word \n",
        "token_word = word_tokenize(para)\n",
        "print(token_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-681rldqUvI",
        "outputId": "a1644f5a-ed81-4825-d930-13d081be9896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ram', 'Krishna', 'Pandey', 'is', 'the', 'student', 'of', 'the', 'college', 'TMSL', '.', 'He', 'is', 'right', 'now', 'LEARNING', \"'NATURAL\", 'LANGUAGE', 'PROCESSING', \"'\", 'with', 'suvidha', 'foundation', '.', 'He', 'lives', 'in', 'Kolakta']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_tokenize(para) :\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZum-QtqsUh",
        "outputId": "d05170a9-4574-455c-a4cc-6f2eceda1f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ram\n",
            "Krishna\n",
            "Pandey\n",
            "is\n",
            "the\n",
            "student\n",
            "of\n",
            "the\n",
            "college\n",
            "TMSL\n",
            ".\n",
            "He\n",
            "is\n",
            "right\n",
            "now\n",
            "LEARNING\n",
            "'NATURAL\n",
            "LANGUAGE\n",
            "PROCESSING\n",
            "'\n",
            "with\n",
            "suvidha\n",
            "foundation\n",
            ".\n",
            "He\n",
            "lives\n",
            "in\n",
            "Kolakta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let us take another example \n",
        "para2 = \"i love coding . i love python\"\n",
        "token_sent = sent_tokenize(para2)\n",
        "token_word = word_tokenize(para2)\n",
        "print(token_sent)   \n",
        "print(token_word)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgK7t4UQs1h5",
        "outputId": "4ef0ed62-f0e7-4fbb-f7d0-33b5f3c94560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i love coding .', 'i love python']\n",
            "['i', 'love', 'coding', '.', 'i', 'love', 'python']\n"
          ]
        }
      ]
    }
  ]
}