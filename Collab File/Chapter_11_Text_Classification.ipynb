{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPFkNPz6_aRu"
      },
      "outputs": [],
      "source": [
        "# Importing the NLTK library and Random module\n",
        "import nltk\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading all the NLTK files\n",
        "nltk.download_shell()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Pp0N0JAD0l",
        "outputId": "4855da52-9634-40f2-ee42-678054409f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Movie Review corpus\n",
        "from nltk.corpus import movie_reviews"
      ],
      "metadata": {
        "id": "8gY5RzHd_mar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an empty list for storing the documents\n",
        "documents = []\n",
        "\n",
        "# For each category in the movie_reviews corpus\n",
        "for category in movie_reviews.categories():\n",
        "\n",
        "  # for each file identifiers in the each category\n",
        "  for fileid in movie_reviews.fileids(category):\n",
        "    \n",
        "    # appending the file identifiers and cateories in a list. (the fileids and categories are stored in a set)\n",
        "    documents.append((list(movie_reviews.words(fileid)), category))"
      ],
      "metadata": {
        "id": "rTcXLEzUAtFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the shuffle function of the random module to shuffle the elements of the documents list.\n",
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "NipHAYKq_7F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the first element (text) of the list of documents\n",
        "print(documents[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMrB5-j5_8GE",
        "outputId": "12b302ce-cb71-4766-a4b0-57d19e0dde8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['a', 'fully', 'loaded', 'entertainment', 'review', '-', 'website', 'coming', 'in', 'july', '!', 'i', 'didn', \"'\", 't', 'really', 'expect', 'very', 'much', 'when', 'i', 'rented', 'stuart', 'saves', 'his', 'family', '.', 'the', 'movie', 'bombed', 'at', 'the', 'box', 'office', ',', 'and', 'i', 'had', 'never', 'really', 'liked', 'the', 'saturday', 'night', 'live', 'sketch', 'on', 'which', 'the', 'movie', 'was', 'based', '.', 'my', 'real', 'concern', ',', 'though', ',', 'was', 'the', 'general', 'reputation', 'of', 'snl', '-', 'related', 'movies', 'translating', 'to', 'the', 'big', 'screen', 'with', 'less', '-', 'than', '-', 'stellar', 'results', '(', 'half', '-', 'baked', ',', 'anyone', '?', ')', 'i', 'was', 'surprised', 'to', 'discover', 'a', 'truly', 'entertaining', ',', 'often', 'hilarious', 'yet', 'also', 'touching', 'little', 'film', '.', 'it', 'is', 'an', 'al', 'franken', 'production', 'through', '-', 'and', '-', 'through', ',', 'who', 'not', 'only', 'created', 'the', 'title', 'character', 'but', 'also', 'wrote', 'and', 'starred', 'in', 'the', 'picture', '.', 'for', 'those', 'of', 'you', 'who', 'don', \"'\", 't', 'know', ',', 'al', 'franken', 'plays', 'a', 'guy', 'named', 'stuart', 'smalley', ',', 'who', 'has', 'a', 'really', 'annoying', 'show', 'on', 'a', 'public', '-', 'access', 'station', 'about', 'increasing', 'your', 'self', '-', 'esteem', ',', 'etc', '.', 'although', 'not', 'a', 'licensed', 'therapist', ',', 'stuart', 'smalley', 'is', ',', 'as', 'the', 'program', 'proudly', 'boasts', ',', '\"', 'a', 'graduate', 'of', 'several', '12', '-', 'step', 'programs', '.', '\"', 'stuart', 'is', 'doing', 'well', 'enough', 'with', 'his', 'tv', 'show', ',', 'but', 'he', 'keeps', 'getting', 'distracted', 'by', 'family', 'troubles', '.', 'stuart', \"'\", 's', 'relatives', 'are', 'a', 'mess', ':', 'his', 'dad', 'and', 'brother', '(', 'harris', 'yulin', 'and', 'vincent', 'd', \"'\", 'onofrio', ',', 'respectively', ')', 'are', 'alcoholics', ',', 'his', 'sister', 'is', 'overweight', 'and', 'getting', 'over', 'another', 'divorce', ',', 'and', 'his', 'mom', 'is', 'in', 'constant', 'self', '-', 'denial', 'and', 'solves', 'all', 'perceived', 'problems', 'by', 'baking', '.', 'then', ',', 'disaster', 'strikes', '.', 'stuart', 'accidentally', 'insults', 'the', 'manager', 'of', 'the', 'station', 'and', 'gets', 'his', 'show', 'revoked', '.', 'he', 'is', 'forced', 'to', 'move', 'back', 'in', 'with', 'his', 'family', 'after', 'he', 'can', 'no', 'longer', 'support', 'himself', ',', 'and', 'there', 'the', 'comedy', 'truly', 'picks', 'up', '.', 'franken', 'wrote', 'all', 'of', 'the', 'characters', 'not', 'just', 'as', 'cliches', 'or', 'stereotypes', ',', 'but', 'as', 'sharply', '-', 'drawn', ',', 'real', 'people', '.', 'although', 'smalley', \"'\", 's', 'dad', 'has', 'always', 'imbibed', 'too', 'much', ',', 'stuart', 'can', 'still', 'recall', 'times', 'when', 'his', 'father', 'seemed', 'like', 'an', 'okay', 'guy', '.', 'also', ',', 'after', 'an', 'accident', 'towards', 'the', 'end', 'of', 'the', 'movie', ',', 'both', 'stuart', 'and', 'the', 'audience', 'learn', 'that', 'there', 'is', 'more', 'to', 'his', 'mother', 'than', 'previously', 'thought', '.', 'although', 'filled', 'with', 'small', ',', 'very', 'funny', 'scenes', ',', 'stuart', 'saves', 'his', 'family', 'also', 'has', 'its', 'share', 'of', 'dramatic', 'moments', ',', 'especially', 'towards', 'the', 'conclusion', '.', 'for', 'a', 'lightweight', 'comedy', ',', 'it', 'handles', 'these', 'scenes', 'especially', 'well', ',', 'often', 'better', 'than', 'many', 'so', '-', 'called', '\"', 'serious', 'dramas', '\"', 'of', 'today', '.', 'all', 'the', 'more', 'amazing', 'is', 'that', 'such', 'a', 'movie', 'comes', 'from', 'a', 'saturday', 'night', 'live', 'sketch', 'that', 'is', 'essentially', 'a', 'one', '-', 'joke', 'bit', '.', 'if', 'you', 'feel', 'like', 'renting', 'a', 'comedy', ',', 'and', 'you', 'haven', \"'\", 't', 'seen', 'this', 'film', ',', 'i', 'definitely', 'recommend', 'you', 'try', 'this', 'one', 'out', '.', 'you', \"'\", 'll', 'laugh', '.', '.', '.', 'you', \"'\", 'll', 'cry', '.', '.', '.', '.', 'it', \"'\", 's', 'all', 'there', '.'], 'pos')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get all the word tokens of the movie_review corpus and stored it in a list (all_words)\n",
        "all_words = []\n",
        "for w in movie_reviews.words():\n",
        "    all_words.append(w.lower()) # lowring the words"
      ],
      "metadata": {
        "id": "oryug7iJ_8uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the frequency distribition of all the words\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "# printing out the 15 most common words\n",
        "print(all_words.most_common(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xeg7le7q_9Cc",
        "outputId": "2bbd98e1-7ecd-4f05-cbc2-f69a9a1dd94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the occurence of the word \"stupid\" in the corpus.\n",
        "print(all_words[\"stupid\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsDOazVdDoWy",
        "outputId": "688647d0-e249-40ef-d8e9-7f3bbf921610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXyXS-EJDrej"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}