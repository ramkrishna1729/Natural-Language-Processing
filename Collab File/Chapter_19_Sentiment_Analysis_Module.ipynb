{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the required modules**"
      ],
      "metadata": {
        "id": "9qHzqY7xLTeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5FIGu0rIQG3"
      },
      "outputs": [],
      "source": [
        "# Importing the NLTK library and Random module\n",
        "import nltk\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2EHKLtzIpwo",
        "outputId": "2fc35bdf-ba34-43e0-cf6e-da332c57fdb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "# Downloading all the NLTK files\n",
        "nltk.download_shell()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4NajwlD4uFN"
      },
      "outputs": [],
      "source": [
        "# Importing tokenizers\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRLOzW7L342l"
      },
      "source": [
        "**Uploading the text files in google colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "63kNZ2xKfGUv",
        "outputId": "2da2ac3a-fc3e-4e32-cc68-4377cd3cacbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-66a0dc13-a876-4a4b-8644-d3e620877436\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-66a0dc13-a876-4a4b-8644-d3e620877436\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving negative.txt to negative.txt\n",
            "Saving positive.txt to positive.txt\n"
          ]
        }
      ],
      "source": [
        "# Improting file from google colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95y37wp94Cl9"
      },
      "source": [
        "**Opening and reading the required text files (short reviews)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCNFq5ieiPvu"
      },
      "outputs": [],
      "source": [
        "# opening and reading the required text files\n",
        "short_positives = open(\"/content/positive.txt\", mode='r', encoding='ISO-8859-1').read()\n",
        "short_negatives = open(\"/content/negative.txt\", mode='r', encoding='ISO-8859-1').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjE_NwWC4J9L"
      },
      "source": [
        "**Storing the short reviews in documents and all word tokens lists**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an empty list for storing the documents\n",
        "documents = []\n",
        "\n",
        "# Creating an empty list for storing appropriate word tokens\n",
        "all_words_tokens = []\n",
        "\n",
        "# Allowing for only adjectives as PoS tag for our reviews\n",
        "allowed_pos_tag = [\"J\"]"
      ],
      "metadata": {
        "id": "5k_ZCKAPwHVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating through each review splitting with a new line in the short_positives\n",
        "for rev in short_positives.split('\\n'):\n",
        "\n",
        "  # appending reviews in the documents list with 'pos' tag\n",
        "  documents.append((rev, \"pos\"))\n",
        "\n",
        "  # creating word tokens of the reviews\n",
        "  word_tokens = word_tokenize(rev)\n",
        "\n",
        "  # pos tagging of the word tokens\n",
        "  pos = nltk.pos_tag(word_tokens)\n",
        "  \n",
        "  # Taking only the allowed pos tags and appending\n",
        "  # them int he all_words_tokens list\n",
        "  for token in pos:\n",
        "    if token[1][0] in allowed_pos_tag:\n",
        "      all_words_tokens.append(token[0].lower())"
      ],
      "metadata": {
        "id": "mwojLG_hzOci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating through each review splitting with a new line in the short_negatives\n",
        "for rev in short_negatives.split('\\n'):\n",
        "\n",
        "  # appending reviews in the documents list with 'pos' tag\n",
        "  documents.append((rev, \"neg\"))\n",
        "\n",
        "  # creating word tokens of the reviews\n",
        "  word_tokens = word_tokenize(rev)\n",
        "\n",
        "  # pos tagging of the word tokens\n",
        "  pos = nltk.pos_tag(word_tokens)\n",
        "  \n",
        "  # Taking only the allowed pos tags and appending\n",
        "  # them int he all_words_tokens list\n",
        "  for token in pos:\n",
        "    if token[1][0] in allowed_pos_tag:\n",
        "      all_words_tokens.append(token[0].lower())"
      ],
      "metadata": {
        "id": "va8GiPsI55d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing pickle module\n",
        "import pickle"
      ],
      "metadata": {
        "id": "8hba_F386qlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the documents list\n",
        "save_docs = open(\"documents.pickle\", \"wb\")\n",
        "pickle.dump(documents, save_docs)\n",
        "save_docs.close()"
      ],
      "metadata": {
        "id": "GJ8fw7Nw7IJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoL9bZI_NBA3"
      },
      "source": [
        "**Frequency Distribution of all the word tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKiJ8eaoI6qB"
      },
      "outputs": [],
      "source": [
        "# Get the frequency distribition of all the words\n",
        "all_words_freq = nltk.FreqDist(all_words_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTwDj90LLJRz",
        "outputId": "ba704fb9-42f7-40ea-cbfc-0440d7f90a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6178\n"
          ]
        }
      ],
      "source": [
        "# Print out the length of the all frequent words list\n",
        "print(len(all_words_freq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG1uexfqna4E",
        "outputId": "9603a6e2-2be3-4a5a-b624-9b5c147bae7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('good', 369),\n",
              " ('more', 331),\n",
              " ('little', 265),\n",
              " ('funny', 245),\n",
              " ('much', 234),\n",
              " ('bad', 234),\n",
              " ('best', 208),\n",
              " ('new', 206),\n",
              " ('own', 185),\n",
              " ('many', 183)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Printing out the top 10 most common words \n",
        "all_words_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the 5000 most common words \n",
        "most_common_word_tokens = all_words_freq.most_common(5000)"
      ],
      "metadata": {
        "id": "QxHpSMLR8PH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L9W2Aa3LfKz",
        "outputId": "c7ea64e8-ecf1-4cff-d682-04bbd8b0a00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'more', 'little', 'funny', 'much', 'bad', 'best', 'new', 'own', 'many']\n"
          ]
        }
      ],
      "source": [
        "# Since the elements of the most_common_word_tokens list are in the form of tuples, \n",
        "# we need to extract the keys of each tuple to get the words as word features\n",
        "word_features = [word[0] for word in most_common_word_tokens]\n",
        "\n",
        "# Print out the top 10 word features\n",
        "print(word_features[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vhfUlG1LfH8",
        "outputId": "3b72eb6a-fce0-4f8f-9360-566166956d19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Length of the word_features list\n",
        "len(word_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the word_features list\n",
        "save_word_features = open(\"word_features5k.pickle\", \"wb\")\n",
        "pickle.dump(word_features, save_word_features)\n",
        "save_word_features.close()"
      ],
      "metadata": {
        "id": "jGe8psDW-sBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw9IVYcNLl-3"
      },
      "source": [
        "**Creating a Feature Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boR_Xr5ULfEx"
      },
      "outputs": [],
      "source": [
        "# Creating a function to get the features (words) in a dictionary\n",
        "def doc_features(doc):\n",
        "    \n",
        "    doc_words = word_tokenize(doc)\n",
        "    \n",
        "    # creating an empty features list\n",
        "    features = {}\n",
        "    \n",
        "    # Will iterate through all the words present in the word_features list\n",
        "    for word in word_features:\n",
        "        \n",
        "        # Get that word and see its presence in the document (will return a bollean value)\n",
        "        features[word] = (word in doc_words)\n",
        "    \n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyg_ynFJLe-L"
      },
      "outputs": [],
      "source": [
        "# Now, we are going to create a feature set which will contain the word features of the review and its correspoding category\n",
        "feature_sets = [(doc_features(review), category) for (review, category) in documents]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffling the feature_sets\n",
        "random.shuffle(feature_sets)"
      ],
      "metadata": {
        "id": "kHu38KsZnPb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving feature sets\n",
        "featuresets_f = open(\"featuresets.pickle\", \"wb\")\n",
        "pickle.dump(feature_sets, featuresets_f)\n",
        "featuresets_f.close()"
      ],
      "metadata": {
        "id": "GKhprTQXGsiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_inAXDG7-pZd",
        "outputId": "d31fe738-98aa-4303-f013-07edfd10cc2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10664"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Length of the feature_sets\n",
        "len(feature_sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzcInCmlSxMi"
      },
      "source": [
        "**Model Training**\n",
        "\n",
        "Now, we will create training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT3xbrqiMxO3"
      },
      "outputs": [],
      "source": [
        "# Training set and Testing set\n",
        "train_data = feature_sets[:8000]\n",
        "test_data = feature_sets[8000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAGGoSKUS5Nx",
        "outputId": "ed5aebb3-7854-412a-827e-e7fee306e326"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Length of training set\n",
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRxLyI7FS6-W",
        "outputId": "ea08c19a-baef-4a65-9eb2-3c48f3df6dc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2664"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Length of testing set\n",
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6jwXl-7S8_p",
        "outputId": "cbe58648-cea1-4187-bba6-159dc8d98e05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7501875468867217, 0.24981245311327832)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "(len(train_data)/len(feature_sets), len(test_data)/len(feature_sets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnkA1fvbTAA3"
      },
      "source": [
        "We will be using the **Naive Bayes Classifier** for our training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x85JoqCDS-Ia"
      },
      "outputs": [],
      "source": [
        "# Importing the NaiveBayesClassifier from nltk\n",
        "from nltk import NaiveBayesClassifier\n",
        "\n",
        "# Creating an instance of our classifier and training the model\n",
        "base_model = NaiveBayesClassifier.train(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwxyrgQES-E7",
        "outputId": "4d7045f5-93f9-46c4-8e0b-707f4b596107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score of Base Model : 72.07207207207207%\n"
          ]
        }
      ],
      "source": [
        "# Importing classify from nltk\n",
        "from nltk import classify\n",
        "\n",
        "# Calculating the accuracy of the base model \n",
        "accuracy_score = classify.accuracy(base_model, test_data)\n",
        "print(\"Accuracy Score of Base Model : {}%\".format(100 * accuracy_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6wfgZYLS-B9",
        "outputId": "fff0204f-b404-47be-9178-cceda8cb7f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "               wonderful = True              pos : neg    =     17.1 : 1.0\n",
            "              engrossing = True              pos : neg    =     16.5 : 1.0\n",
            "               inventive = True              pos : neg    =     14.4 : 1.0\n",
            "                 routine = True              neg : pos    =     12.2 : 1.0\n",
            "                powerful = True              pos : neg    =     12.2 : 1.0\n",
            "                    imax = True              pos : neg    =     11.8 : 1.0\n",
            "                  sexual = True              pos : neg    =     11.8 : 1.0\n",
            "             masterpiece = True              pos : neg    =     11.1 : 1.0\n",
            "             mesmerizing = True              pos : neg    =     11.1 : 1.0\n",
            "                    loud = True              neg : pos    =     10.9 : 1.0\n",
            "                  boring = True              neg : pos    =     10.6 : 1.0\n",
            "              refreshing = True              pos : neg    =     10.4 : 1.0\n",
            "                    flat = True              neg : pos    =     10.1 : 1.0\n",
            "                touching = True              pos : neg    =      9.9 : 1.0\n",
            "                   urban = True              pos : neg    =      9.7 : 1.0\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Show 15 most informative features\n",
        "print(base_model.show_most_informative_features(15))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving base_model (Naive Bayes Classifier)\n",
        "save_classifier = open(\"base_model_naivebayes5k.pickle\",\"wb\")\n",
        "pickle.dump(base_model, save_classifier)\n",
        "save_classifier.close()"
      ],
      "metadata": {
        "id": "B5piKR-JAYgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Other Classifiers** "
      ],
      "metadata": {
        "id": "XSwJlh4RvOoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPGJVGBnTWsR"
      },
      "outputs": [],
      "source": [
        "# Importing scikit-learn module from NLTK (a wrapper for sklearn)\n",
        "from nltk.classify.scikitlearn import SklearnClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUctZnGu31m1"
      },
      "outputs": [],
      "source": [
        "# Lets use some other types of Naive Bayes classifiers from sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "\n",
        "# Lets import some more classifiers\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w6iQG5v32tb",
        "outputId": "94e0b8f3-5bec-4026-8680-a880e953a002"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(MultinomialNB())>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Multinomial Naive Bayes classifier\n",
        "multinomial_nb_model = SklearnClassifier(MultinomialNB())\n",
        "multinomial_nb_model.train(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOuew33s32mk",
        "outputId": "6a88d076-25ee-4fc0-8e90-b3bd7b025001"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(BernoulliNB())>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Bernoulli Naive Bayes classifier\n",
        "bernoulli_nb_model = SklearnClassifier(BernoulliNB())\n",
        "bernoulli_nb_model.train(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlekKPU532ju",
        "outputId": "e70110a0-0ec8-42cf-e745-a7f2a35e92ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(SGDClassifier())>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Stochastic Gradient Descent classifier\n",
        "sgd_model = SklearnClassifier(SGDClassifier())\n",
        "sgd_model.train(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqkxjbQZ32cW",
        "outputId": "54411bc1-5e37-4129-f0e7-e0bf3b9d19e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(LinearSVC())>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Linear Support Vector Classification classifier\n",
        "linear_svc_model = SklearnClassifier(LinearSVC())\n",
        "linear_svc_model.train(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nu-Support Vector Classification classifier\n",
        "nu_svc_model = SklearnClassifier(NuSVC())\n",
        "nu_svc_model.train(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1hXamg7AGGu",
        "outputId": "0f9e788e-d275-4bcd-eeb6-2d639cc8e8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(NuSVC())>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uwsfZ2Y32Wx",
        "outputId": "e05954c1-e032-4536-f76b-f1cdf4a102b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score of Base Model (Naive Bayes) : 72.07207207207207%\n",
            "MultinomialNB Model Accuracy Score: 71.69669669669669%\n",
            "BernoulliNB Model Accuracy Score: 71.47147147147147%\n",
            "SGDClassifier Model Accuracy Score: 70.08258258258259%\n",
            "LinearSVC Model Accuracy Score: 68.88138138138137%\n",
            "NuSVC Model Accuracy Score: 71.05855855855856%\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy Score of Base Model (Naive Bayes) : {}%\".format(100 * accuracy_score))\n",
        "print(\"MultinomialNB Model Accuracy Score: {}%\".format(100 * classify.accuracy(multinomial_nb_model, test_data)))\n",
        "print(\"BernoulliNB Model Accuracy Score: {}%\".format(100 * classify.accuracy(bernoulli_nb_model, test_data)))\n",
        "print(\"SGDClassifier Model Accuracy Score: {}%\".format(100 * classify.accuracy(sgd_model, test_data)))\n",
        "print(\"LinearSVC Model Accuracy Score: {}%\".format(100 * classify.accuracy(linear_svc_model, test_data)))\n",
        "print(\"NuSVC Model Accuracy Score: {}%\".format(100 * classify.accuracy(nu_svc_model, test_data)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Multinomial Naive Bayes classifier\n",
        "save_classifier = open(\"multinomial_nb_model5k.pickle\",\"wb\")\n",
        "pickle.dump(multinomial_nb_model, save_classifier)\n",
        "save_classifier.close()"
      ],
      "metadata": {
        "id": "hkmiK5IAFAZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Bernoulli Naive Bayes classifier\n",
        "save_classifier = open(\"bernoulli_nb_model5k.pickle\",\"wb\")\n",
        "pickle.dump(bernoulli_nb_model, save_classifier)\n",
        "save_classifier.close()"
      ],
      "metadata": {
        "id": "Dx3b-RhNFBNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Stochastic Gradient Descent classifier\n",
        "save_classifier = open(\"sgd_model5k.pickle\",\"wb\")\n",
        "pickle.dump(sgd_model, save_classifier)\n",
        "save_classifier.close()"
      ],
      "metadata": {
        "id": "4ebFonlbFB3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Linear Support Vector Classification classifier\n",
        "save_classifier = open(\"linear_svc_model5k.pickle\",\"wb\")\n",
        "pickle.dump(linear_svc_model, save_classifier)\n",
        "save_classifier.close()"
      ],
      "metadata": {
        "id": "dmK93VxXFCXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Nu-Support Vector Classification classifier\n",
        "save_classifier = open(\"nu_svc_model5k.pickle\",\"wb\")\n",
        "pickle.dump(nu_svc_model, save_classifier)\n",
        "save_classifier.close()"
      ],
      "metadata": {
        "id": "mdZ276-1GSPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After creating the sentiment module and saving it as sentiment_module.py, we will now import the module**"
      ],
      "metadata": {
        "id": "_axnchBm19PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "zpqTlPH8RTwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/NLP\\ Internship\\ /Data/sentiment_module.py /content"
      ],
      "metadata": {
        "id": "IC3S0uqARdwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the sentiment moduel\n",
        "import sentiment_module as sm"
      ],
      "metadata": {
        "id": "T686DGkfUBDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878949a8-4f6a-46eb-ad26-ef2647b89f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the method of the sentiment module\n",
        "sm.give_sentiment(\"This is an awesome movie. The direction was brilliant, the acting was perfect and I loved every bit of it.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q1HprY82Q4S",
        "outputId": "82d7e071-929f-46d8-85a9-b34c5b2e9779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm.give_sentiment(\"This is a bad movie. I didn't like it at all. The acting was not that good and the videography was utter disappointment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHtEbaR14QkC",
        "outputId": "3f35bf37-8d17-40fb-9f18-2e91c3667efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg', 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm.give_sentiment(\"I understand the language was meant for the broad audience to understand but the producers could have made a better attempt at having the cast take on a heavier Italian accent. That was strike one for me. The acting was not atrocious but the script was unnecessarily extended in many acts, strike two. Too many blips in the storyline. There was no smooth transition of the time period progressions of Ferrucio's life, strike three. I know this was a direct to video movie but there was a lot of potential, A LOT of potential for this and it was a complete failure. Do better. Hopefully a different director/producer/writer/studio can portray the life of Lamborghini appropriately. You already are beat with the new Ferrari movie that isn't even out yet.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTphBQhX4VNv",
        "outputId": "be77916b-2bdb-48a6-dc9a-bb210760c620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg', 0.8)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm.give_sentiment(\"very beautiful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhUtzYUNOgS2",
        "outputId": "cf167530-6e0d-43aa-e8ff-9b07d1ee7ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm.give_sentiment(\"good film\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQnLt8dbRevN",
        "outputId": "48ca1da5-44c4-4398-c273-271e091d5c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg', 0.6)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm.give_sentiment(\"This was the best movie.\")"
      ],
      "metadata": {
        "id": "zPL5ECaF4gJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7f9c15-ad16-478b-a460-59b8afc057a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sm.give_sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))\n",
        "print(sm.give_sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
      ],
      "metadata": {
        "id": "u5cWpLCLR2SZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2774156e-c279-4b08-8482-7d7e6d7a8f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('pos', 1.0)\n",
            "('neg', 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RlDY7fhOVjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}