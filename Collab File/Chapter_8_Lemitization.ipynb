{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Cgcn30tPov",
        "outputId": "63d1ff7f-f6bd-4365-cf70-c44331c87b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#importing nltk libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer #importing library for lemitaization "
      ],
      "metadata": {
        "id": "yP3jdWhXufJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \" Success is Not Final, Failure is Not Fatal: it is the Courage to Continue that Counts.\""
      ],
      "metadata": {
        "id": "vdeUWxcbuxhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(sent)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYXnGU9XvgIO",
        "outputId": "d698213a-4059-43f4-c0c9-c98a836352e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Success', 'is', 'Not', 'Final', ',', 'Failure', 'is', 'Not', 'Fatal', ':', 'it', 'is', 'the', 'Courage', 'to', 'Continue', 'that', 'Counts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining lemmatizing  library\n",
        "lemm = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "0gBs2LRzwiTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#running the word token in loop so that each and every words grts lemitize\n",
        "for word in words: \n",
        "  print(lemm.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJyxru0rvGxY",
        "outputId": "8b4c5e2c-61e4-4495-da08-fe92ed499791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n",
            "is\n",
            "Not\n",
            "Final\n",
            ",\n",
            "Failure\n",
            "is\n",
            "Not\n",
            "Fatal\n",
            ":\n",
            "it\n",
            "is\n",
            "the\n",
            "Courage\n",
            "to\n",
            "Continue\n",
            "that\n",
            "Counts\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#running the word token in loop so that each and every words grts lemitize\n",
        "for word in words: \n",
        "  print(lemm.lemmatize(word , pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZQv29qhziaP",
        "outputId": "11d5b735-b75a-405d-b667-531d5619d50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success\n",
            "is\n",
            "Not\n",
            "Final\n",
            ",\n",
            "Failure\n",
            "is\n",
            "Not\n",
            "Fatal\n",
            ":\n",
            "it\n",
            "is\n",
            "the\n",
            "Courage\n",
            "to\n",
            "Continue\n",
            "that\n",
            "Counts\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import  PorterStemmer\n",
        "stemm = PorterStemmer()"
      ],
      "metadata": {
        "id": "TmrlNncz0DSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#differnce between lemitizing and stemming\n",
        "for word in words:\n",
        "    print(\"{}'s stem is {}.\".format(word, stemm.stem(word)))\n",
        "    print(\"{}'s lemma is {}.\".format(word, lemm.lemmatize(word)))\n",
        "    print('\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R58NgdkzZm2",
        "outputId": "ff45da05-c81d-4671-e7d1-d6c51229efd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success's stem is success.\n",
            "Success's lemma is Success.\n",
            "\n",
            "\n",
            "is's stem is is.\n",
            "is's lemma is is.\n",
            "\n",
            "\n",
            "Not's stem is not.\n",
            "Not's lemma is Not.\n",
            "\n",
            "\n",
            "Final's stem is final.\n",
            "Final's lemma is Final.\n",
            "\n",
            "\n",
            ",'s stem is ,.\n",
            ",'s lemma is ,.\n",
            "\n",
            "\n",
            "Failure's stem is failur.\n",
            "Failure's lemma is Failure.\n",
            "\n",
            "\n",
            "is's stem is is.\n",
            "is's lemma is is.\n",
            "\n",
            "\n",
            "Not's stem is not.\n",
            "Not's lemma is Not.\n",
            "\n",
            "\n",
            "Fatal's stem is fatal.\n",
            "Fatal's lemma is Fatal.\n",
            "\n",
            "\n",
            ":'s stem is :.\n",
            ":'s lemma is :.\n",
            "\n",
            "\n",
            "it's stem is it.\n",
            "it's lemma is it.\n",
            "\n",
            "\n",
            "is's stem is is.\n",
            "is's lemma is is.\n",
            "\n",
            "\n",
            "the's stem is the.\n",
            "the's lemma is the.\n",
            "\n",
            "\n",
            "Courage's stem is courag.\n",
            "Courage's lemma is Courage.\n",
            "\n",
            "\n",
            "to's stem is to.\n",
            "to's lemma is to.\n",
            "\n",
            "\n",
            "Continue's stem is continu.\n",
            "Continue's lemma is Continue.\n",
            "\n",
            "\n",
            "that's stem is that.\n",
            "that's lemma is that.\n",
            "\n",
            "\n",
            "Counts's stem is count.\n",
            "Counts's lemma is Counts.\n",
            "\n",
            "\n",
            ".'s stem is ..\n",
            ".'s lemma is ..\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}